# YOLO object detection hands-on seminar.
An introduction to using the [ultralytics/yolov5](https://github.com/ultralytics/yolov5) repository for object detection.

## Scope
---
This repository should only be considered as the minimal introduction for someone looking to use yolov5 for the following tasks.
* Run inference on a set of sample images using one of the pretrained model.
* Run training on a custom dataset.
* Understand the evaluation metrics generated by the yolo training pipeline.

Note: The official repository already has quite a good tutorial notebooks to start with, you may consider taking a look at the following as well.
* [Object detection](https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb)
* [Image Segmentation](https://github.com/ultralytics/yolov5/blob/master/segment/tutorial.ipynb)
* [Classification](https://github.com/ultralytics/yolov5/blob/master/classify/tutorial.ipynb)

## Setup
---
We do not clone the yolov5 repository but rather use the [pypi package](https://pypi.org/project/yolov5/).

Setup a virtual environment. I personally prefer the conda environment, create venv in conda using the below command.
```
conda create -n yolo_seminar python=3.7
```
Install the requirements in your environment.
```
pip install -r requirements.txt
```

## Downloading sample dataset
---
We download a sample dataset provided by the ultralytics repository itself. A full list of datasets and scripts to download them are available [here](https://github.com/ultralytics/yolov5/tree/master/data).

We will use the `coco128` dataset that contains 128 images from the coco dataset. In order to download the dataset to desired location, you may run the following command.
```
sh data/scripts/get_coco128.sh
```
Please note that the above script is taken from the official repository directly.

### Understanding the Annotations format.
For each image there corresponds a label `.txt` file which contains all the annotations of that image.

Each annotation is a separate row with the format `class x_center y_center width height`.
For more details please refer this section [here](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#12-create-labels-1).

### Creating dataset `yaml` config file.
The training pipeline expects a config file that describes the class mappings and also mentions the dataset path. For coco128 dataset, we define the config file at path `data/coco128.yaml`.

Please make sure that you change the `path` inside the yaml file to the absolute local path where dataset persists.

Please note that this config file is taken from the official repository directly.

## Running model inference
---
In order to run the predictions on a set of images, the module provides a script which could be directly used with user configurable command line arguments.

You need to choose a pretrained model checkpoint from the [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints) of pretrained models that exist.

In the argument `--weights` you should mention the corresponding names among the model configs present [here](https://github.com/ultralytics/yolov5/tree/master/models). Eg. `yolov5s.pt` or `yolov5m6.pt` (present inside the hub directory).

We could simply mention the name of the model checkpoint and it automatically downloads from the above list of model checkpoints.

```
python -m yolov5.detect \
--weights='yolov5s.pt' \
--source='data/datasets/coco128/images/train2017' \
--data='data/coco128.yaml' \
--img 640 \
--conf-thres=0.4 \
--iou-thres=0.5 \
--max-det=1000 \
--save-txt \
--save-conf \
--agnostic-nms \
--project='prediction' \
--name='coco128'
```
Running the above command runs the predictions on all the images inside the `--source` path and persists inside the `--project` path.

We could use a lot of plug and play configurations when running the prediction script. Refer the [detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py#L219) for a detailed description on user configurable parameters.

### Prediction result format.
By default, running the above script persists images with predicted bounding box and class label in the `--project` path but we could also get the predictions in a detailed format persisted per image in a `.txt` format, if `--save-txt` flag is true.

Each text file corresponds to the predictions of a specific image and each line represents a prediction. Each line is of the format.<br> `class_id x_center y_center width height score_conf`<br>
x_center, y_center, width, height are normalized values from 0 to 1.

## Train object detection model.
The official documentation provides [details](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data) to keep in mind to train the model for your custom dataset. However, we here try to run the training with minimum configuration.

```
python -m yolov5.train \
--weights='yolov5s.pt' \
--data='data/coco128.yaml' \
--epochs=300 \
--batch-size=16 \
--img=640 \
--multi-scale \
--optimizer='AdamW' \
--project='training' \
--name='coco128' \
--cos-lr \
--save-period=1 \
--cache
```
Recommended - Refer the [Tips for Best Training Results](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results) section to explore in-depth configurations for better training experiments.

## Checking Evaluation Stats.
The yolo API tracks the loss for each step and logs to tensorboard. It also gives evaluations stats which one can use to pick most relevant score confidence threshold at inference time. A few of the key plots are:
* F1 vs score
* PR vs score
* Confusion metrics
* Loss curves for train/val (helps to estimate overfitting)